<!DOCTYPE html>
<html lang="en">
    <head>
        <title>The task of face liveness detection</title>
        <link rel="stylesheet" href="../css/styles.css" type="text/css">
        <link rel="icon" href="../images/icon.png">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <a href="https://oxid15.github.io/">
            <img class="index_logo" src="../images/square_dark.png">
        </a>
        <nav>
            <ul class="top-menu">
                <li><a href="https://oxid15.github.io/index.html#about">About</a></li>
                <li><a href="https://oxid15.github.io/index.html#projects">Projects</a></li>
                <li><a href="https://oxid15.github.io/index.html#contacts">Contacts</a></li>
                <li><a class="important-link" href="https://oxid15.github.io/posts.html">Posts</a></li>
            </ul>
        </nav>
        <div style="clear: both;"></div>
        
        <div class="title-container">
            <h1 class="title" id="title">The task of face liveness detection</h1>
        </div>
        <h5 class="subtitle">08.11.2020</h5>
        
        <div class="text-container"> 
            Review outline:
            <nav>
                <ul class="table-of-contents">
                    <li><a href="#whatis">What is face liveness detection</a></li>
                    <li><a href="#types">Types of approaches</a></li>
                    <li><a href="#texture">Texture analysis</a></li>
                    <li><a href="#motion">Motion analysis</a></li>
                    <li><a href="#life">Life signs</a></li>
                    <li><a href="#data">Datasets</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                    <li><a href="#references">References</a></li>
                </ul>
            </nav>
            <p>
                <span class="letter">B</span>iometric systems become more and more widespread in recent years. 
                2D face recognition is not an exception, being one of the most fast growing biometric technologies 
                for different applications from surveillance to smartphone unlocking. It is not surprising due to minimum
                effort that needed from user to use those systems along with the minimum amount of special devices needed.<br>
                The main problem with this systems, which is also the main line of attack, is <span class="emp-text">unability 
                for it to distinguish between the actual face and the image of the face</span> in front of the camera.
            </p>

            <h3 id="whatis">What is face liveness detection</h3>
            <p>
                <span class="letter">A</span>s you already guessed, face liveness detection task (also known as 
                face spoofing detection) is to determine whether we have a real face in
                front of the camera, or its fake copy. The copy being an image of the face printed on the paper or showed 
                on the screen, rubber mask or even a makeup, that was made to make other face look similar. Every serious
                face recognition system is capable of doing that. But how it classifies between real and fake faces? We would
                talk about that in the following sections.
            </p>

            <img class="image" src="../images/posts/the-task-of-face-liveness-detection/faces.png">
            <p class="centered-text">Can you guess which photo was taken from a real face and which was taken from another photo? 
            That's the difficult task even for a human. Illustration from <a href="#1">[1]</a>. The answer: the rightmost column
            are the real faces.
            </p>

            <h3 id="types">Types of approaches</h3>
            <p>
                <span class="letter">A</span>ll variety of different approaches to the task of face liveness detection can be divided into three folloving categories: 
                texture analysis, motion analysis and life signs detection.<br>
                <span class="emp-text">Texture analysis</span> aims to catch local texture differences between
                real and fake faces. It is based on the usage of pixel values in some neighborhoods as they are or in terms of Fourier spectrum.
                The usage of Local Binary Pattern or LBP is a popular technique within this kind of approaches. LBP is a powerful texture descriptor, that
                encodes any neighborhood of pixel as a binary number.<br>
                <span class="emp-text">Motion analysis</span> methods estimate motion field of the demonstrated face: is it similar to the motion field of
                a flat piece of paper? The real face have a specific motion field, which is drastically different from a field that is generated by a flat photo.<br>
                <span class="emp-text">Life signs analysis</span> is also based on the analysis of video sequence. This category includes a wide variety of
                approaches from the eye movement detection to the analysis of lip movement. 
                The methods that require user unteraction are also fall into this category:
                they ask user to perform certain action to prove that the face is real.
            </p>

            <h3 id="texture">Texture analysis</h3>
            <p>
                <span class="letter">T</span>exture analysis is the most popular category of approaches due to the simplicity of implementation and
                modest time consumption. This field is based on the improvements of pattern recognition approaches. To further discuss the approaches in
                this field, let's look at the <span class="emp-text">concept of LBP.</span><br>
                The approaches of texture analysis assume that input data would have the same size, orientation and intensity properties as the training data.
                However, in practice this isn't always true. The most of real world data can have different spatial resolution, can be rotated or have different
                lighting conditions. All of this circumstances could lead to degraded performance of face liveness classificator. To address this kind of issues,
                T. Ojala et al. applied a concept of Local Binary Patterns in the paper <a href="#2">[2]</a>. Basically, LBP assigns a binary number to each
                pixel of the image, forming a feature map with unique numbers for different types of textures. Here is how it works.<br>
                First of all, we form circle shaped neighborhood as it shown on the image below. 
                <img class="image" src="../images/posts/the-task-of-face-liveness-detection/lbp.png"> 
                <p class="centered-text">The coordinates of the points are calculated as: 
                    <p class="math">\( (-R sin(\frac{2\pi p}{P}), R cos(\frac{2\pi p}{P})) \)</p> 
                </p>
                Then if the value in certain point of this neighborhood is larger than central pixel's value, 
                we place 1 in the corresponding bit. If the value is smaller, we
                place 0 in that bit. This approach allows us to assign individual 
                number to each pixel, that describes its neighborhood texture and at the same time is invariant
                to uniform intensity shifts. The formula, describing this process looks like this.
                <p class="math">\( LBP_{P, R} = \sum_{p=0}^{P-1} \limits s(g_p - g_c)2^p \)</p>
                Where \(s\) is the signal function that returns 1 if x is greater or equal to zero and 0 if x is less than zero.<br>
                Invariance to intesity shifts is great, but what about rotations? If we rotate the same texture pattern, we will got
                different LBPs. To address this, the authors also mention another descriptor: <span class="emp-text">rotation invariant LBP</span>. It can be formulated
                as follows: let's "rotate" LBP until it's value would be minimun possible number. If we take differently rotated texture
                and apply this descriptor to it, we would have the same result every time.<br>
                LBPs are important texture descriptors that allow us to get texture information that is resistant to rotations and constant intensity
                shifts.<br><br>
                Now we can take a look at how this metric could be used in the task of face liveness detection. In the work <a href="#3">[3]</a>
                the authors use multiscale LBP histograms in combination with red-green deviation and block-based color moment as a features,
                to achieve high accuracy on different datasets. Red-green deviation is the difference between LBP histograms of red and green channels,
                 which is different in real and fake faces, due to the inherent characteristics of the face, and block-based color moment is the
                set of statistical moments calculated for different local blocks of the image. All of this features were fed into SVM to get the final
                answer about initial face image.
                <img class="image" src="../images/posts/the-task-of-face-liveness-detection/skinbloodflow.png">
                <p class="centered-text">The pipeline of the model from <a href="#3">[3]</a></p>
            </p>

            <h3 id="motion">Motion analysis</h3>
            <p>
                <span class="letter">T</span>he work of Bao W. et al. <a href="#4">[4]</a> was about how to use the information about the motion to tackle
                the problem of face liveness detection. Theoretical basis of this paper states that every motion of planar object could be described as 
                linear combination of translation, rotation, moving forward or backward and swing (perpendicular to the axis of viewing). The point is that
                human face is a complex object that is far from being planar. In addition to that, the motion of fourth kind is more valuable - it
                is better at discriminaing faces from planar objects. 
                <br>
                <br>
                <img src="../images/posts/the-task-of-face-liveness-detection/motion.png">
                <p class="centered-text"> Motion fields of flat image, curled image and real face</p>
                Why did we said that fourth kind of motion is more valuable? Let's look at the motion fields that planar object would produce when being
                moved in all four ways.
                <img class="image" src="../images/posts/the-task-of-face-liveness-detection/planar.png">
                We can see the how face could differ in fourh case right away - it would produce <b>more</b> motion in center, where the nose is and 
                <b>less</b> motion on the sides. The authors decided to use this difference to discriminate between live and fake faces. So their 
                strategy implies deciding whether motion field belongs to a planar object - the image of the face, or a complex object - the face itself.
            </p>

            <h3 id="life">Life signs</h3>
            <p>
                <span class="letter">L</span>ife signs are the specific movements that human face makes, for which artificial copies of it are not capable.
                It could be eye blinking, mouth movement or head movement for example.
                This kind of features are more high-level than
                described before. These methods include the ones that require user interaction and the ones that don't. 
                First kind of methods (that can also be called intrusive) are the ones that ask the users to perform certain actions to prove that they are real.
                They provide more accuracy and hence more security, but consume more time and effort from user. Which means that they are suitable for high 
                security facilities, but wouldn't be so suitable for frequent usage in fields where risk is not so high. Take smartphone unlocking for example.
                <br>
                The next method we describe belongs to the second category. It is based on the <span class="emp-text">detection of blinking</span> <a href="#5">[5]</a>. 
                Authors of this work used probabillistic model, which is widespread use in natural language processing, 
                called conditional random fields or <span class="emp-text">CRF</span>. 
                This model is frequently used for data sequences and formulates the blinking process as two states - opened and closed.
                <img class="image" src="../images/posts/the-task-of-face-liveness-detection/crf.png">
                So the main task for this kind of classification is to precisely detect the blinking, because blinking considered to be the behavior that
                photos are not capable. Are they? Well, there exist attacks that are aimed to the kinds of systems that use blinking or mouth movement
                in their decision making process. The idea is to use the picture of the face with eyes and mouth cropped out to simulate blinking and 
                mouth movement with the real eyes and mouth.
                <img class="image" src="../images/posts/the-task-of-face-liveness-detection/attack.png">
                <p class="centered-text"><a href="http://rose1.ntu.edu.sg/Datasets/faceLivenessDetection.asp">ROSE-Youtu Face Liveness Detection Dataset</a> 
                has the images with that kind of attack.</p>
                So there exist attacks on this kind of methods, but in combination with texture- or motion-based methods it could be much stronger, albeit slower.
            </p>

            <h3 id="data">Datasets</h3>
            <p>
                <span class="letter">R</span>ecent methods in the field of liveness detection (as in every other field in computer vision now) are learning-based.
                That means that the algorithms strongly <span class="emp-text">dependent on the data</span> they are trained on. 
                What datasets are there to train model on? There are several options.<br><br>
                <span class="emp-text">Idiap Replay-Attack Database <a href="#6">[6]</a></span> being the most popular, 
                this dataset consist of 1300 colored videos of real faces
                of different persons and attacks
                in the form of printed image or playing video sequence. Embedded laptop 240x320 webcam was used 
                to capture .mov videos. During the recording two sets of 
                lighting were used: "controlled lighting conditions" (with office lighting, curtains are down, background is uniform) and "adverse lighting conditions"
                (without office lighting, curtains are up, background is complex).<br><br>
                <span class="emp-text">CASIA-FASD (Chinese Academy of Sciences, Institute of Automation Face Anti-spoofing Database)<a href="#7">[7]</a></span>.
                This dataset consist of real and fake faces of 50 persons. The attacks are performed using warped images and images with eyes cropped out.
                The data was captured using 640х480, 480х640 and 1280х720 cameras.<br><br>
                <span class="emp-text">NUAA Photograph Imposter Database (Nanjing University of Aeronautics and Astronautics)<a href="#8">[8]</a></span>
                consists of 12000 face photos of 15 different users under different light conditions. It differs from another datasets in the way that it is
                free for non-commercial use. The authors provide three versions of this dataset: original images, crops containing only faces, obtained with
                face detector and geometrically normalized gray-scale images.
            </p>

            <h3 id="conclusion">Conclusion</h3>
            <p>
                <span class="letter">W</span>hat can we say in conclusion? This post is trying to provide basic understanding of face liveness detection problem and
                the ways of solving it. What are the main problems that are present in this field? I would consider diverse lighting conditions, image noise of different
                nature, which is corrupt texture information and the presence of glasses, face masks, etc. These problems require not only 
                the engineer's effort to address them, but also the presence of big and diverse datasets, that should include images not only with diverse light, resolution,
                skin color of users, user's mimics, but also a set of attacks against face liveness detection system such as cropped out eyes and mouth, rubber masks, 
                high-resolution printed photos, makeup, etc.

            </p>

            <h3 id="references">References</h3>
            <nav>
                <ul class="references">
                    <li id="1">
                        X. Tan, Y. Li, J. Liu, and L. Jiang, “Face liveness detection from a single image with sparse low rank bilinear discriminative model,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 6316 LNCS, no. PART 6, pp. 504–517, 2010.
                    </li>

                    <li id="2">
                        T. Ojala, M. Pietikäinen, and T. Maenpaa, “Multiresolution gray-scale and rotation invariant texture classification with local binary patterns.,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, 2002.
                    </li>

                    <li id="3">
                        S. Y. Wang, S. H. Yang, Y. P. Chen, and J. W. Huang, “Face liveness detection based on skin blood flow analysis,” Symmetry (Basel)., vol. 9, no. 12, pp. 1–18, 2017.
                    </li>

                    <li id="4">
                        W. Bao, H. Li, N. Li, and W. Jiang, “A liveness detection method for face recognition based on optical flow field,” Proc. 2009 Int. Conf. Image Anal. Signal Process. IASP 2009, pp. 233–236, 2009.
                    </li>

                    <li id="5">
                        L. Sun, G. Pan, Z. Wu, and S. Lao, “Blinking-Based Live Face Detection Using Conditional Random Fields,” Adv. Biometrics, pp. 252–260, 2007.
                    </li>

                    <li id="6">
                        I. Chingovska, A. Anjos, and S. Marcel, "On the effectiveness of local binary patterns in face anti-spoofing." 2012 BIOSIG-proceedings of the international conference of biometrics special interest group (BIOSIG). IEEE, 2012.
                    </li>

                    <li id="7">
                        Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Li "A Face Antispoofing Database with Diverse Attacks," In proceedings of the 5th IAPR International Conference on Biometrics (ICB'12), pp. 26–31, 2012.
                    </li>

                    <li id="8">
                        X. Tan, Y. Li, J. Liu, and L. Jiang "Face liveness detection from a single image with sparse low rank bilinear discriminative model," Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), no. 6316 LNCS, PART 6, pp. 504–517, 2010.
                    </li>

                </ul>
            </nav>
        </div>
    </body>
</html>