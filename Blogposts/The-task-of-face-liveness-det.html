<!DOCTYPE html>
<html style="font-size: 16px;">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="​Review of image smoothing algorithms">
    <meta name="description" content="">
    <meta name="page_type" content="np-template-header-footer-from-plugin">
    <title>The task of face liveness det</title>
    <link rel="stylesheet" href="../styles.css" media="screen">
<link rel="stylesheet" href="The-task-of-face-liveness-det.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../script.js" defer=""></script>
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web:200,200i,300,300i,400,400i,600,600i,700,700i,900|Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "../images/icon1.png"
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta property="og:title" content="The task of face liveness det">
    <meta property="og:type" content="website">
  </head>
  <body class="u-body"><header class="u-black u-clearfix u-header u-header" id="sec-ec72"><div class="u-clearfix u-sheet u-sheet-1">
        <a href="../index.html" data-page-id="46058780" class="u-image u-logo u-image-1" data-image-width="550" data-image-height="550" title="Home">
          <img src="../images/icon1.png" class="u-logo-image u-logo-image-1">
        </a>
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-text-active-color u-custom-text-hover-color u-custom-top-bottom-menu-spacing u-grey-75 u-nav-link u-text-active-palette-1-base u-text-custom-color-2 u-text-hover-palette-2-base" href="#" style="font-size: calc(1em + 14px); padding: 7px 10px;">
              <svg viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><symbol id="menu-hamburger" viewBox="0 0 16 16" style="width: 16px; height: 16px;"><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</symbol>
</defs></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-white u-text-custom-color-2 u-text-hover-white" href="../index.html" style="padding: 10px 6px 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-white u-text-custom-color-2 u-text-hover-white" href="../Posts.html" style="padding: 10px 6px 10px 20px;">Posts</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../index.html" style="padding: 10px 6px 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Posts.html" style="padding: 10px 6px 10px 20px;">Posts</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-clearfix u-section-1" id="sec-fbfb">
      <div class="u-align-left u-clearfix u-sheet u-sheet-1">
        <h1 class="u-text u-text-default u-text-1"> The task of face liveness detection</h1>
        <p class="u-text u-text-2"> 08.11.2020</p>
      </div>
    </section>
    <section class="u-clearfix u-section-2" id="sec-aada">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h2 class="u-text u-text-default u-text-1"> Review outline:</h2>
        <p class="u-text u-text-default u-text-2">
          <a href="Review-of-Img-Smooth-Algos.html#carousel_58c7" data-page-id="195068502" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1"></a>
          <a href="The-task-of-face-liveness-det.html#carousel_58c7" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-2">What is face liveness detection<br>
          </a>
          <a href="The-task-of-face-liveness-det.html#carousel_ad0e" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-3">Types of approaches</a>
          <br>&nbsp; &nbsp; <a href="The-task-of-face-liveness-det.html#carousel_2ae9" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-4">Texture analysis</a>
          <br>&nbsp; &nbsp; <a href="The-task-of-face-liveness-det.html#carousel_425f" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-5">Motion analysis</a>
          <br>&nbsp; &nbsp; <a href="The-task-of-face-liveness-det.html#sec-a264" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-6">Life signs</a>
          <br>
          <a href="The-task-of-face-liveness-det.html#carousel_a6ae" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-7">Datasets</a>
          <br>
          <a href="The-task-of-face-liveness-det.html#carousel_8596" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-8">Conclusion</a>
          <br>
          <a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-9">References</a>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-3" id="sec-884b">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1"> Biometric systems become more and more widespread in recent years. 2D face recognition is not an exception, being one of the most fast growing biometric technologies for different applications from surveillance to smartphone unlocking. It is not surprising due to minimum effort that needed from user to use those systems along with the minimum amount of special devices needed.<br>The main problem with this systems, which is also the main line of attack, is&nbsp;unability for it to distinguish between the actual face and the image of the face&nbsp;in front of the camera.
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-4" id="carousel_58c7">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h2 class="u-text u-text-default u-text-1"> What is face liveness detection</h2>
        <p class="u-text u-text-2"> As you already guessed, face liveness detection task (also known as face spoofing detection) is to determine whether we have a real face in front of the camera, or its fake copy. The copy being an image of the face printed on the paper or showed on the screen, rubber mask or even a makeup, that was made to make other face look similar. Every serious face recognition system is capable of doing that. But how it classifies between real and fake faces? We would talk about that in the following sections.</p>
        <img class="u-image u-image-default u-image-1" src="../images/faces.png" alt="" data-image-width="826" data-image-height="341">
        <p class="u-text u-text-3"> Can you guess which photo was taken from a real face and which was taken from another photo? That's the difficult task even for a human. Illustration from&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[1]</a>. The answer: the rightmost column are the real faces.
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-5" id="carousel_ad0e">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h2 class="u-text u-text-default u-text-1"> Types of approaches</h2>
        <p class="u-text u-text-2"> All variety of different approaches to the task of face liveness detection can be divided into three folloving categories: texture analysis, motion analysis and life signs detection.<br>Texture analysis&nbsp;aims to catch local texture differences between real and fake faces. It is based on the usage of pixel values in some neighborhoods as they are or in terms of Fourier spectrum. The usage of Local Binary Pattern or LBP is a popular technique within this kind of approaches. LBP is a powerful texture descriptor, that encodes any neighborhood of pixel as a binary number.<br>Motion analysis&nbsp;methods estimate motion field of the demonstrated face: is it similar to the motion field of a flat piece of paper? The real face have a specific motion field, which is drastically different from a field that is generated by a flat photo.<br>Life signs analysis&nbsp;is also based on the analysis of video sequence. This category includes a wide variety of approaches from the eye movement detection to the analysis of lip movement. The methods that require user unteraction are also fall into this category: they ask user to perform certain action to prove that the face is real.
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-6" id="carousel_2ae9">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h3 class="u-text u-text-default u-text-1"> Texture analysis</h3>
        <p class="u-text u-text-2"> Texture analysis is the most popular category of approaches due to the simplicity of implementation and modest time consumption. This field is based on the improvements of pattern recognition approaches. To further discuss the approaches in this field, let's look at the&nbsp;concept of LBP.<br>The approaches of texture analysis assume that input data would have the same size, orientation and intensity properties as the training data. However, in practice this isn't always true. The most of real world data can have different spatial resolution, can be rotated or have different lighting conditions. All of this circumstances could lead to degraded performance of face liveness classificator. To address this kind of issues, T. Ojala et al. applied a concept of Local Binary Patterns in the paper&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[2]</a>. Basically, LBP assigns a binary number to each pixel of the image, forming a feature map with unique numbers for different types of textures. Here is how it works.<br>First of all, we form circle shaped neighborhood as it shown on the image below.
        </p>
        <img class="u-image u-image-default u-image-1" src="../images/lbp.png" alt="" data-image-width="1528" data-image-height="372">
        <p class="u-text u-text-default u-text-3"> The coordinates of the points are calculated as: -R sin(2pi*p / P), R cos(2pi*p / P)</p>
      </div>
    </section>
    <section class="u-clearfix u-section-7" id="carousel_2fa8">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <p class="u-align-left u-text u-text-1"> Then if the value in certain point of this neighborhood is larger than central pixel's value, we place 1 in the corresponding bit. If the value is smaller, we place 0 in that bit. This approach allows us to assign individual number to each pixel, that describes its neighborhood texture and at the same time is invariant to uniform intensity shifts. The formula, describing this process looks like this.</p>
        <img class="u-image u-image-default u-preserve-proportions u-image-1" src="../images/ql_6ded788405a2e84c6e5efe4cd93f0189_l3.png" alt="" data-image-width="247" data-image-height="53">
        <p class="u-align-left u-text u-text-2"> Where&nbsp;s&nbsp;is the signal function that returns 1 if x is greater or equal to zero and 0 if x is less than zero.<br>Invariance to intesity shifts is great, but what about rotations? If we rotate the same texture pattern, we will got different LBPs. To address this, the authors also mention another descriptor:&nbsp;rotation invariant LBP. It can be formulated as follows: let's "rotate" LBP until it's value would be minimun possible number. If we take differently rotated texture and apply this descriptor to it, we would have the same result every time.<br>LBPs are important texture descriptors that allow us to get texture information that is resistant to rotations and constant intensity shifts.<br>
          <br>Now we can take a look at how this metric could be used in the task of face liveness detection. In the work&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[3]</a>&nbsp;the authors use multiscale LBP histograms in combination with red-green deviation and block-based color moment as a features, to achieve high accuracy on different datasets. Red-green deviation is the difference between LBP histograms of red and green channels, which is different in real and fake faces, due to the inherent characteristics of the face, and block-based color moment is the set of statistical moments calculated for different local blocks of the image. All of this features were fed into SVM to get the final answer about initial face image.
        </p>
        <img class="u-align-left u-expanded-width u-image u-image-default u-image-2" src="../images/skinbloodflow.png" alt="" data-image-width="1341" data-image-height="325">
        <p class="u-align-left u-text u-text-3"> The pipeline of the model from&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-2">[3]</a>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-8" id="carousel_425f">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h3 class="u-align-left u-text u-text-1"> Motion analysis</h3>
        <p class="u-align-left u-text u-text-2"> The work of Bao W. et al.&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[4]</a>&nbsp;was about how to use the information about the motion to tackle the problem of face liveness detection. Theoretical basis of this paper states that every motion of planar object could be described as linear combination of translation, rotation, moving forward or backward and swing (perpendicular to the axis of viewing). The point is that human face is a complex object that is far from being planar. In addition to that, the motion of fourth kind is more valuable - it is better at discriminaing faces from planar objects.
        </p>
        <img class="u-image u-image-default u-image-1" src="../images/motion.png" alt="" data-image-width="920" data-image-height="336">
        <p class="u-align-left u-text u-text-3"> Motion fields of flat image, curled image and real face</p>
        <p class="u-align-left u-text u-text-4"> Why did we said that fourth kind of motion is more valuable? Let's look at the motion fields that planar object would produce when being moved in all four ways.</p>
        <img class="u-image u-image-default u-image-2" src="../images/planar.png" alt="" data-image-width="816" data-image-height="376">
        <p class="u-align-left u-text u-text-5"> We can see the how face could differ in fourh case right away - it would produce&nbsp;<b>more</b>&nbsp;motion in center, where the nose is and&nbsp;<b>less</b>&nbsp;motion on the sides. The authors decided to use this difference to discriminate between live and fake faces. So their strategy implies deciding whether motion field belongs to a planar object - the image of the face, or a complex object - the face itself.
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-9" id="sec-a264">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h3 class="u-text u-text-default u-text-1"> Life signs</h3>
        <p class="u-text u-text-2">Life signs are the specific movements that human face makes, for which artificial copies of it are not capable. It could be eye blinking, mouth movement or head movement for example. This kind of features are more high-level than described before. These methods include the ones that require user interaction and the ones that don't. First kind of methods (that can also be called intrusive) are the ones that ask the users to perform certain actions to prove that they are real. They provide more accuracy and hence more security, but consume more time and effort from user. Which means that they are suitable for high security facilities, but wouldn't be so suitable for frequent usage in fields where risk is not so high. Take smartphone unlocking for example.<br>The next method we describe belongs to the second category. It is based on the&nbsp;detection of blinking&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[5]</a>. Authors of this work used probabillistic model, which is widespread use in natural language processing, called conditional random fields or&nbsp;CRF. This model is frequently used for data sequences and formulates the blinking process as two states - opened and closed.
        </p>
        <img class="u-image u-image-default u-image-1" src="../images/crf.png" alt="" data-image-width="772" data-image-height="424">
      </div>
    </section>
    <section class="u-clearfix u-section-10" id="sec-91be">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <p class="u-align-left u-text u-text-1"> So the main task for this kind of classification is to precisely detect the blinking, because blinking considered to be the behavior that photos are not capable. Are they? Well, there exist attacks that are aimed to the kinds of systems that use blinking or mouth movement in their decision making process. The idea is to use the picture of the face with eyes and mouth cropped out to simulate blinking and mouth movement with the real eyes and mouth.</p>
        <img class="u-image u-image-default u-image-1" src="../images/attack.png" alt="" data-image-width="632" data-image-height="141">
        <p class="u-align-left u-text u-text-2">
          <a href="http://rose1.ntu.edu.sg/Datasets/faceLivenessDetection.asp" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">ROSE-Youtu Face Liveness Detection Dataset</a>&nbsp;has the images with that kind of attack.<br>
        </p>
        <p class="u-align-left u-text u-text-3"> So there exist attacks on this kind of methods, but in combination with texture- or motion-based methods it could be much stronger, albeit slower.</p>
      </div>
    </section>
    <section class="u-clearfix u-section-11" id="carousel_a6ae">
      <div class="u-clearfix u-sheet u-valign-middle-sm u-valign-middle-xs u-sheet-1">
        <h2 class="u-align-left u-text u-text-1"> Datasets</h2>
        <p class="u-align-left u-text u-text-2"> Recent methods in the field of liveness detection (as in every other field in computer vision now) are learning-based. That means that the algorithms strongly&nbsp;dependent on the data&nbsp;they are trained on. What datasets are there to train model on? There are several options.</p>
        <p class="u-align-left u-text u-text-3"> Idiap Replay-Attack Database&nbsp;<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">[6]</a>&nbsp;being the most popular, this dataset consist of 1300 colored videos of real faces of different persons and attacks in the form of printed image or playing video sequence. Embedded laptop 240x320 webcam was used to capture .mov videos. During the recording two sets of lighting were used: "controlled lighting conditions" (with office lighting, curtains are down, background is uniform) and "adverse lighting conditions" (without office lighting, curtains are up, background is complex).<br>
          <br>CASIA-FASD (Chinese Academy of Sciences, Institute of Automation Face Anti-spoofing Database)<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-2">[7]</a>. This dataset consist of real and fake faces of 50 persons. The attacks are performed using warped images and images with eyes cropped out. The data was captured using 640х480, 480х640 and 1280х720 cameras.<br>
          <br>NUAA Photograph Imposter Database (Nanjing University of Aeronautics and Astronautics)<a href="The-task-of-face-liveness-det.html#carousel_e6c8" data-page-id="93972749" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-3">[8]</a>&nbsp;consists of 12000 face photos of 15 different users under different light conditions. It differs from another datasets in the way that it is free for non-commercial use. The authors provide three versions of this dataset: original images, crops containing only faces, obtained with face detector and geometrically normalized gray-scale images.<br>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-12" id="carousel_8596">
      <div class="u-clearfix u-sheet u-valign-middle-lg u-valign-middle-xl u-sheet-1">
        <h2 class="u-align-left u-text u-text-1"> Conclusion</h2>
        <p class="u-align-left u-text u-text-2"> What can we say in conclusion? This post is trying to provide basic understanding of face liveness detection problem and the ways of solving it. What are the main problems that are present in this field? I would consider diverse lighting conditions, image noise of different nature, which is corrupt texture information and the presence of glasses, face masks, etc. These problems require not only the engineer's effort to address them, but also the presence of big and diverse datasets, that should include images not only with diverse light, resolution, skin color of users, user's mimics, but also a set of attacks against face liveness detection system such as cropped out eyes and mouth, rubber masks, high-resolution printed photos, makeup, etc.</p>
      </div>
    </section>
    <section class="u-clearfix u-section-13" id="carousel_e6c8">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h2 class="u-align-left u-text u-text-1"> References</h2>
        <ol class="u-align-left u-text u-text-2">
          <li>X. Tan, Y. Li, J. Liu, and L. Jiang, “Face liveness detection from a single image with sparse low rank bilinear discriminative model,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 6316 LNCS, no. PART 6, pp. 504–517, 2010.</li>
          <li>T. Ojala, M. Pietikäinen, and T. Maenpaa, “Multiresolution gray-scale and rotation invariant texture classification with local binary patterns.,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, 2002.</li>
          <li>S. Y. Wang, S. H. Yang, Y. P. Chen, and J. W. Huang, “Face liveness detection based on skin blood flow analysis,” Symmetry (Basel)., vol. 9, no. 12, pp. 1–18, 2017.</li>
          <li>W. Bao, H. Li, N. Li, and W. Jiang, “A liveness detection method for face recognition based on optical flow field,” Proc. 2009 Int. Conf. Image Anal. Signal Process. IASP 2009, pp. 233–236, 2009.</li>
          <li>L. Sun, G. Pan, Z. Wu, and S. Lao, “Blinking-Based Live Face Detection Using Conditional Random Fields,” Adv. Biometrics, pp. 252–260, 2007.</li>
          <li>I. Chingovska, A. Anjos, and S. Marcel, "On the effectiveness of local binary patterns in face anti-spoofing." 2012 BIOSIG-proceedings of the international conference of biometrics special interest group (BIOSIG). IEEE, 2012.</li>
          <li>Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Li "A Face Antispoofing Database with Diverse Attacks," In proceedings of the 5th IAPR International Conference on Biometrics (ICB'12), pp. 26–31, 2012.</li>
          <li>X. Tan, Y. Li, J. Liu, and L. Jiang "Face liveness detection from a single image with sparse low rank bilinear discriminative model," Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), no. 6316 LNCS, PART 6, pp. 504–517, 2010.</li>
        </ol>
      </div>
    </section>
    
    
    <footer class="u-align-center u-black u-clearfix u-footer u-footer" id="sec-550d"><div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <p class="u-small-text u-text u-text-variant u-text-1">Ilia Moiseev | <a href="mailto:ilia.moiseev.5@yandex.ru" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-2 u-btn-1">ilia.moiseev.5@yandex.ru</a>
        </p>
      </div></footer>
  </body>
</html>